# LLMPapers

LLM: Large Language Model

## [Content](#content)

<table>
<tr><td colspan="2"><a href="#survey-papers">1. Survey</a></td></tr> 
<tr><td colspan="2"><a href="#fundamental-papers">2. Fundamental Papers</a></td></tr> 
<tr><td colspan="2"><a href="#methods">3. Methods</a></td></tr>
<tr>
    <td>&emsp;<a href="#in-context learning">3.1 In-context Learning</a></td>
    <td></td>
</tr>
<tr><td colspan="2"><a href="#fine-tuning">4. Fine-tuning</a></td></tr>
<tr>
    <td>&emsp;<a href="#post-ft">2.1 Post FT</a></td>
    <td></td>
</tr>
<tr><td colspan="2"><a href="#quantization">5. Quantization</a></td></tr>
<tr><td colspan="2"><a href="#applications">6. Applications</a></td></tr> 
<tr>
    <td>&emsp;<a href="#qa">6.1 QA</a></td>
    <td></td>
</tr>
<tr><td colspan="2"><a href="#cool-ideas">7. Cool Ideas</a></td></tr>
<tr><td colspan="2"><a href="#llm-explainability">8. LLM Explainability</a></td></tr>
<tr>
    <td></td>
    <td></td>
</tr>
<tr><td colspan="2"><a href="#useful-notebooks and repos">9. Useful Notebooks and Repos</a></td></tr>
</table>

## [Survey papers](#content)


## [Fundamental papers](#content)
1. **Attention is all you need.** NeuIPS 2017. [paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

    *Vaswani, Ashish, et al.*
   
## [Methods](#content)   

### [In-context Learning](#content)
1. **n-Context Learning Creates Task Vectors.** arXiv, 2023. [paper](https://arxiv.org/pdf/2310.15916.pdf)
    *Hendel, Roee, Mor Geva, and Amir Globerson.*
1. **Iterative Forward Tuning Boosts In-context Learning in Language Models** arXiv, 2023. [paper](https://arxiv.org/pdf/2305.13016.pdf)
   *Yang, Jiaxi, et al.*

## [Fine-tuning](#content) 
1.  **How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.** arXiv, 2023. [paper](https://arxiv.org/pdf/2310.05492.pdf)
	*Dong, G., et al.*


### [Post FT](#content) 

1. **Direct Preference Optimization: Your Language Model is Secretly a Reward
  Model.** arXiv, 2023. [paper](https://arxiv.org/pdf/2305.18290.pdf)

    *Rafailov, R., et al.*


## [Quantization](#content)  
1. **LongLLMLingua: ACCELERATING AND ENHANCING LLMS IN LONG CONTEXT SCENARIOS VIA PROMPT COMPRESSION** EMNLP, 2023. [paper](https://arxiv.org/pdf/2310.06839.pdf) [Code](https://github.com/microsoft/LLMLingua)

## [Applications](#content)  

### [QA](#content)
1. **Merging Generated and Retrieved Knowledge for Open-Domain QA.** arXiv, 2023. [paper](https://arxiv.org/pdf/2310.14393.pdf)
   *Zhang, Y., et al. *
	
## [Cool Ideas](#content)  
1. **RAIN: Your Language Models Can Align Themselves without Finetuning.** arXiv, 2023. [paper](https://arxiv.org/pdf/2309.07124.pdf) [Code](https://github.com/SafeAILab/RAIN)
    *Li, Y., et al.*

1. **Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs** arXiv, 2023. [paper](https://arxiv.org/pdf/2311.02262.pdf) [Code](https://github.com/QingruZhang/PASTA)

## [LLM Explainability](#content)  

## [Useful Notebooks and Repos](#content)  
1. **Fine-tuning Mistral 7B using QLoRA** [Code](https://github.com/brevdev/notebooks/blob/main/mistral-finetune.ipynb)
1. **OCR + Amazon's MistralLite for a PDF Analysis Chatbot.** [Code](https://github.com/brevdev/notebooks/blob/main/ocr-pdf-analysis.ipynb)
1. **Hallucination Leaderboard** [Code](https://github.com/vectara/hallucination-leaderboard/)
1. **LLM Guardrails/Hallucination Evaluation** [Code](https://confluence.ext.net.nokia.com/pages/viewpage.action?pageId=1578607869)
